{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Optimization of LlamaIndex RAG Pipeline: A Step-by-Step Approach\n",
    "\n",
    "Llama new website: https://docs.llamaindex.ai/en/stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deeplake in ./.venv/lib/python3.10/site-packages (3.8.22)\n",
      "Requirement already satisfied: llama_index in ./.venv/lib/python3.10/site-packages (0.10.16)\n",
      "Requirement already satisfied: langchain in ./.venv/lib/python3.10/site-packages (0.1.11)\n",
      "Requirement already satisfied: openai in ./.venv/lib/python3.10/site-packages (1.13.3)\n",
      "Requirement already satisfied: tiktoken in ./.venv/lib/python3.10/site-packages (0.6.0)\n",
      "Requirement already satisfied: cohere in ./.venv/lib/python3.10/site-packages (4.53)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (2.2.1)\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-2.6.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from deeplake) (1.26.4)\n",
      "Requirement already satisfied: pillow in ./.venv/lib/python3.10/site-packages (from deeplake) (10.2.0)\n",
      "Requirement already satisfied: boto3 in ./.venv/lib/python3.10/site-packages (from deeplake) (1.34.34)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.10/site-packages (from deeplake) (8.1.7)\n",
      "Requirement already satisfied: pathos in ./.venv/lib/python3.10/site-packages (from deeplake) (0.3.2)\n",
      "Requirement already satisfied: humbug>=0.3.1 in ./.venv/lib/python3.10/site-packages (from deeplake) (0.3.2)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (from deeplake) (4.66.2)\n",
      "Requirement already satisfied: lz4 in ./.venv/lib/python3.10/site-packages (from deeplake) (4.3.3)\n",
      "Requirement already satisfied: pyjwt in ./.venv/lib/python3.10/site-packages (from deeplake) (2.8.0)\n",
      "Requirement already satisfied: pydantic in ./.venv/lib/python3.10/site-packages (from deeplake) (2.6.3)\n",
      "Requirement already satisfied: libdeeplake==0.0.104 in ./.venv/lib/python3.10/site-packages (from deeplake) (0.0.104)\n",
      "Requirement already satisfied: aioboto3>=10.4.0 in ./.venv/lib/python3.10/site-packages (from deeplake) (12.3.0)\n",
      "Requirement already satisfied: nest-asyncio in ./.venv/lib/python3.10/site-packages (from deeplake) (1.6.0)\n",
      "Requirement already satisfied: dill in ./.venv/lib/python3.10/site-packages (from libdeeplake==0.0.104->deeplake) (0.3.8)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.2.0,>=0.1.4 in ./.venv/lib/python3.10/site-packages (from llama_index) (0.1.5)\n",
      "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in ./.venv/lib/python3.10/site-packages (from llama_index) (0.1.7)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.16 in ./.venv/lib/python3.10/site-packages (from llama_index) (0.10.16.post1)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in ./.venv/lib/python3.10/site-packages (from llama_index) (0.1.6)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 in ./.venv/lib/python3.10/site-packages (from llama_index) (0.1.3)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in ./.venv/lib/python3.10/site-packages (from llama_index) (0.9.48)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.5 in ./.venv/lib/python3.10/site-packages (from llama_index) (0.1.7)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in ./.venv/lib/python3.10/site-packages (from llama_index) (0.1.4)\n",
      "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in ./.venv/lib/python3.10/site-packages (from llama_index) (0.1.4)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in ./.venv/lib/python3.10/site-packages (from llama_index) (0.1.3)\n",
      "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in ./.venv/lib/python3.10/site-packages (from llama_index) (0.1.8)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse<0.2.0,>=0.1.2 in ./.venv/lib/python3.10/site-packages (from llama_index) (0.1.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.10/site-packages (from langchain) (2.0.28)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.10/site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in ./.venv/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.venv/lib/python3.10/site-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.10/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.25 in ./.venv/lib/python3.10/site-packages (from langchain) (0.0.27)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.29 in ./.venv/lib/python3.10/site-packages (from langchain) (0.1.30)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in ./.venv/lib/python3.10/site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in ./.venv/lib/python3.10/site-packages (from langchain) (0.1.22)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./.venv/lib/python3.10/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.10/site-packages (from openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.10/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./.venv/lib/python3.10/site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.10/site-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: backoff<3.0,>=2.0 in ./.venv/lib/python3.10/site-packages (from cohere) (2.2.1)\n",
      "Requirement already satisfied: fastavro<2.0,>=1.8 in ./.venv/lib/python3.10/site-packages (from cohere) (1.9.4)\n",
      "Requirement already satisfied: importlib_metadata<7.0,>=6.0 in ./.venv/lib/python3.10/site-packages (from cohere) (6.11.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in ./.venv/lib/python3.10/site-packages (from cohere) (2.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Collecting transformers<5.0.0,>=4.32.0 (from sentence-transformers)\n",
      "  Using cached transformers-4.39.1-py3-none-any.whl.metadata (134 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Using cached torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Using cached scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Using cached scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in ./.venv/lib/python3.10/site-packages (from sentence-transformers) (0.21.3)\n",
      "Requirement already satisfied: aiobotocore==2.11.2 in ./.venv/lib/python3.10/site-packages (from aiobotocore[boto3]==2.11.2->aioboto3>=10.4.0->deeplake) (2.11.2)\n",
      "Requirement already satisfied: botocore<1.34.35,>=1.33.2 in ./.venv/lib/python3.10/site-packages (from aiobotocore==2.11.2->aiobotocore[boto3]==2.11.2->aioboto3>=10.4.0->deeplake) (1.34.34)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in ./.venv/lib/python3.10/site-packages (from aiobotocore==2.11.2->aiobotocore[boto3]==2.11.2->aioboto3>=10.4.0->deeplake) (1.16.0)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in ./.venv/lib/python3.10/site-packages (from aiobotocore==2.11.2->aiobotocore[boto3]==2.11.2->aioboto3>=10.4.0->deeplake) (0.11.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in ./.venv/lib/python3.10/site-packages (from boto3->deeplake) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in ./.venv/lib/python3.10/site-packages (from boto3->deeplake) (0.10.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.venv/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: zipp>=0.5 in ./.venv/lib/python3.10/site-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.venv/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.9.15)\n",
      "Requirement already satisfied: llama-index-vector-stores-chroma<0.2.0,>=0.1.1 in ./.venv/lib/python3.10/site-packages (from llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.1.5)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.16->llama_index) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.16->llama_index) (1.0.8)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.13 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.16->llama_index) (0.1.13)\n",
      "Requirement already satisfied: networkx>=3.0 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.16->llama_index) (3.2.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in ./.venv/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.16->llama_index) (3.8.1)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in ./.venv/lib/python3.10/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (4.12.3)\n",
      "Requirement already satisfied: bs4<0.0.3,>=0.0.2 in ./.venv/lib/python3.10/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (0.0.2)\n",
      "Requirement already satisfied: pymupdf<2.0.0,>=1.23.21 in ./.venv/lib/python3.10/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (1.23.26)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in ./.venv/lib/python3.10/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (4.1.0)\n",
      "Requirement already satisfied: llama-parse<0.4.0,>=0.3.3 in ./.venv/lib/python3.10/site-packages (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama_index) (0.3.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.10/site-packages (from pydantic->deeplake) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in ./.venv/lib/python3.10/site-packages (from pydantic->deeplake) (2.16.3)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.venv/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.2.0 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./.venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.32.0->sentence-transformers)\n",
      "  Using cached safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: ppft>=1.7.6.8 in ./.venv/lib/python3.10/site-packages (from pathos->deeplake) (1.7.6.8)\n",
      "Requirement already satisfied: pox>=0.3.4 in ./.venv/lib/python3.10/site-packages (from pathos->deeplake) (0.3.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.16 in ./.venv/lib/python3.10/site-packages (from pathos->deeplake) (0.70.16)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (2.5)\n",
      "Requirement already satisfied: chromadb<0.5.0,>=0.4.22 in ./.venv/lib/python3.10/site-packages (from llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.4.24)\n",
      "Requirement already satisfied: onnxruntime<2.0.0,>=1.17.0 in ./.venv/lib/python3.10/site-packages (from llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (1.17.1)\n",
      "Requirement already satisfied: PyMuPDFb==1.23.22 in ./.venv/lib/python3.10/site-packages (from pymupdf<2.0.0,>=1.23.21->llama-index-readers-file<0.2.0,>=0.1.4->llama_index) (1.23.22)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: build>=1.0.3 in ./.venv/lib/python3.10/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (1.1.1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in ./.venv/lib/python3.10/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in ./.venv/lib/python3.10/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.110.0)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in ./.venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.27.1)\n",
      "Requirement already satisfied: posthog>=2.4.0 in ./.venv/lib/python3.10/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (3.5.0)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in ./.venv/lib/python3.10/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (3.4.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in ./.venv/lib/python3.10/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in ./.venv/lib/python3.10/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in ./.venv/lib/python3.10/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in ./.venv/lib/python3.10/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (1.23.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in ./.venv/lib/python3.10/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in ./.venv/lib/python3.10/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in ./.venv/lib/python3.10/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (6.1.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in ./.venv/lib/python3.10/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (1.62.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in ./.venv/lib/python3.10/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (4.1.2)\n",
      "Requirement already satisfied: typer>=0.9.0 in ./.venv/lib/python3.10/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.9.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in ./.venv/lib/python3.10/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (29.0.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in ./.venv/lib/python3.10/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (4.1.0)\n",
      "Requirement already satisfied: coloredlogs in ./.venv/lib/python3.10/site-packages (from onnxruntime<2.0.0,>=1.17.0->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./.venv/lib/python3.10/site-packages (from onnxruntime<2.0.0,>=1.17.0->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (23.5.26)\n",
      "Requirement already satisfied: protobuf in ./.venv/lib/python3.10/site-packages (from onnxruntime<2.0.0,>=1.17.0->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (4.25.3)\n",
      "Requirement already satisfied: pyproject_hooks in ./.venv/lib/python3.10/site-packages (from build>=1.0.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (1.0.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in ./.venv/lib/python3.10/site-packages (from build>=1.0.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (2.0.1)\n",
      "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in ./.venv/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.36.3)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in ./.venv/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (2.28.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in ./.venv/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (1.7.0)\n",
      "Requirement already satisfied: requests-oauthlib in ./.venv/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (1.3.1)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in ./.venv/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (3.2.2)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in ./.venv/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (1.62.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.23.0 in ./.venv/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.23.0 in ./.venv/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.44b0 in ./.venv/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.44b0 in ./.venv/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.44b0 in ./.venv/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.44b0 in ./.venv/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.44b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in ./.venv/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (65.5.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in ./.venv/lib/python3.10/site-packages (from opentelemetry-instrumentation-asgi==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (3.7.2)\n",
      "Requirement already satisfied: monotonic>=1.5 in ./.venv/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (1.6)\n",
      "Requirement already satisfied: httptools>=0.5.0 in ./.venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in ./.venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in ./.venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./.venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in ./.venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (12.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./.venv/lib/python3.10/site-packages (from coloredlogs->onnxruntime<2.0.0,>=1.17.0->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (10.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.venv/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.venv/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./.venv/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in ./.venv/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama_index) (0.5.1)\n",
      "Downloading sentence_transformers-2.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.1/163.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
      "Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hUsing cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.39.1-py3-none-any.whl (8.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\n",
      "Using cached Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, threadpoolctl, scipy, safetensors, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, MarkupSafe, scikit-learn, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, transformers, torch, sentence-transformers\n",
      "Successfully installed MarkupSafe-2.1.5 jinja2-3.1.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 safetensors-0.4.2 scikit-learn-1.4.1.post1 scipy-1.12.0 sentence-transformers-2.6.0 threadpoolctl-3.4.0 torch-2.2.1 transformers-4.39.1 triton-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install deeplake llama_index langchain openai tiktoken cohere pandas torch sentence-transformers llama-index-llms-litellm llama-index-embeddings-cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents: 1\n",
      "Number of nodes: 61 with the current chunk size of 512\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# First we create Document LlamaIndex objects from the text data\n",
    "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
    "node_parser = SimpleNodeParser.from_defaults(chunk_size=512)\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "# By default, the node/chunks ids are set to random uuids. To ensure same id's per run, we manually set them.\n",
    "for idx, node in enumerate(nodes):\n",
    "    node.id_ = f\"node_{idx}\"\n",
    "\n",
    "print(f\"Number of Documents: {len(documents)}\")\n",
    "print(f\"Number of nodes: {len(nodes)} with the current chunk size of {node_parser.chunk_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vector store index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshat/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.8.27) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_15773/786536339.py:16: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm,)\n",
      "/home/akshat/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating embeddings: 100%|██████████| 61/61 [00:03<00:00, 18.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading data to deeplake dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 61/61 [00:00<00:00, 383.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='./data/paul_graham/deep_lake_db', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype      shape      dtype  compression\n",
      "  -------    -------    -------    -------  ------- \n",
      "   text       text      (61, 1)      str     None   \n",
      " metadata     json      (61, 1)      str     None   \n",
      " embedding  embedding  (61, 1536)  float32   None   \n",
      "    id        text      (61, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, ServiceContext, StorageContext\n",
    "from llama_index.vector_stores.deeplake import DeepLakeVectorStore\n",
    "\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Create a local Deep Lake VectorStore\n",
    "dataset_path = \"./data/paul_graham/deep_lake_db\"\n",
    "vector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=True)\n",
    "\n",
    "# LLM that will answer questions with the retrieved context\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "# We use OpenAI's embedding model \"text-embedding-ada-002\"\n",
    "embed_model = OpenAIEmbedding()\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm,)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "vector_index = VectorStoreIndex(nodes, service_context=service_context, storage_context=storage_context, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before college, the main things Paul worked on outside of school were writing and programming. He wrote short stories and experimented with programming on the IBM 1401, using an early version of Fortran.\n"
     ]
    }
   ],
   "source": [
    "query_engine = vector_index.as_query_engine(similarity_top_k=10)\n",
    "response_vector = query_engine.query(\"What are the main things Paul worked on before college?\")\n",
    "print(response_vector.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddigns for qa pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import generate_question_context_pairs\n",
    "\n",
    "qc_dataset = None\n",
    "\n",
    "if not os.path.exists(\"qc_dataset.json\"):\n",
    "    qc_dataset = generate_question_context_pairs(\n",
    "        nodes,\n",
    "        llm=llm,\n",
    "        num_questions_per_chunk=1\n",
    "    )\n",
    "    # We can save the dataset as a json file for later use.\n",
    "    qc_dataset.save_json(\"qc_dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embeddings dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n",
    "\n",
    "if os.path.exists(\"qc_dataset.json\"):\n",
    "    qc_dataset = EmbeddingQAFinetuneDataset.from_json(\n",
    "        \"qc_dataset.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_QA_GENERATE_PROMPT_TMPL = \"\"\"\\\n",
    "Context information is below.\n",
    "\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "\n",
    "Given the context information and not prior knowledge.\n",
    "generate only questions based on the below query.\n",
    "\n",
    "You are a Teacher/ Professor. Your task is to setup \\\n",
    "{num_questions_per_chunk} questions for an upcoming \\\n",
    "quiz/examination. The questions should be diverse in nature \\\n",
    "across the document. Restrict the questions to the \\\n",
    "context information provided.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Both the below metrics are useful for evaluating the effectiveness of a retrieval system, like how well a search engine or a recommendation system works.\n",
    "\n",
    "1. **Hit Rate:** When guessing a correct option from a list of options. The hit rate measures how often you guess the correct answer by only looking at your top few guesses. You have a high hit rate if you often find the right answer in your first few guesses. So, in a retrieval system, **it's about how frequently the system finds the correct document within its top 'k' picks** (where 'k' is a number you decide, like top 5 or top 10).\n",
    "\n",
    "2. **Mean Reciprocal Rank (MRR):** MRR is like measuring how quickly you can find a treasure in a list of boxes. Imagine having a row of boxes and if the treasure is in the 1st box then MMR is 1. If its in 2nd then 1/2 and for nth treasure box its 1/n. The MMR scores for each retrieval is avg at the end. **MMR looks at where the correct doc ranks in the system's guesses**.\n",
    "\n",
    "- See Blog on [Hit Rate & MRR](https://tamilselvan-subramanian.medium.com/how-hit-rate-and-mrr-measure-llm-retrievers-ai-simplified-series-7203ba2d4032#:~:text=Remember%3A-,Hit%20Rate%20tells%20you%20if%20the%20LLM%20found%20any%20relevant,the%20future%20of%20information%20access.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def display_results_retriever(name, eval_results: list, show_list: bool= False) -> pd.DataFrame:\n",
    "    \"\"\"Display results from evaluate\"\"\"\n",
    "\n",
    "    metric_dicts = [eval_result.metric_vals_dict  for eval_result in eval_results]\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "    if show_list:\n",
    "        print(list(full_df['hit_rate']))\n",
    "        print(list(full_df['mrr']))\n",
    "\n",
    "    hit_rate = full_df[\"hit_rate\"].mean()\n",
    "    mrr = full_df[\"mrr\"].mean()\n",
    "\n",
    "    metric_df = pd.DataFrame(\n",
    "        {\"Retriever Name\": [name], \"Hit Rate\": [hit_rate], \"MRR\": [mrr]}\n",
    "    )\n",
    "\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
      "[1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.5, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.5, 1.0, 0.5, 1.0, 0.5, 0.5, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.5, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0]\n",
      "    Retriever Name  Hit Rate       MRR\n",
      "0  Retriever top_2  0.609589  0.520548\n",
      "[1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]\n",
      "[1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.5, 0.5, 1.0, 0.0, 1.0, 0.25, 0.5, 0.25, 1.0, 1.0, 0.3333333333333333, 0.25, 0.5, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.3333333333333333, 0.3333333333333333, 1.0, 0.25, 0.5, 1.0, 0.5, 0.3333333333333333, 0.25, 1.0, 1.0, 1.0, 0.25, 1.0, 0.5, 1.0, 1.0, 1.0, 0.25, 0.3333333333333333, 1.0, 0.5, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.5, 1.0, 0.5, 1.0, 0.5, 0.5, 0.5, 0.0, 1.0, 0.3333333333333333, 0.0, 0.25, 0.5, 0.3333333333333333, 1.0, 0.25, 0.3333333333333333, 0.0, 1.0, 0.0, 0.5, 0.25, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.5, 0.5, 0.3333333333333333, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.25, 1.0, 1.0, 0.25, 0.25, 0.0, 0.5, 0.0]\n",
      "    Retriever Name  Hit Rate       MRR\n",
      "0  Retriever top_4  0.815068  0.581621\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]\n",
      "[1.0, 1.0, 0.16666666666666666, 0.3333333333333333, 1.0, 0.5, 0.5, 1.0, 0.16666666666666666, 1.0, 0.25, 0.5, 0.25, 1.0, 1.0, 0.3333333333333333, 0.25, 0.5, 0.2, 1.0, 0.0, 0.3333333333333333, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.3333333333333333, 0.3333333333333333, 1.0, 0.25, 0.5, 1.0, 0.5, 0.3333333333333333, 0.25, 1.0, 1.0, 1.0, 0.25, 1.0, 0.5, 1.0, 1.0, 1.0, 0.25, 0.3333333333333333, 1.0, 0.5, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.5, 1.0, 0.5, 1.0, 0.5, 0.5, 0.5, 0.2, 1.0, 0.3333333333333333, 0.16666666666666666, 0.25, 0.5, 0.3333333333333333, 1.0, 0.25, 0.3333333333333333, 0.0, 1.0, 0.0, 0.5, 0.25, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.16666666666666666, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.5, 0.5, 0.3333333333333333, 0.16666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.2, 0.2, 0.2, 0.3333333333333333, 1.0, 0.2, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.25, 1.0, 1.0, 0.25, 0.25, 0.0, 0.5, 0.2]\n",
      "    Retriever Name  Hit Rate       MRR\n",
      "0  Retriever top_6   0.89726  0.596918\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]\n",
      "[1.0, 1.0, 0.16666666666666666, 0.3333333333333333, 1.0, 0.5, 0.5, 1.0, 0.16666666666666666, 1.0, 0.25, 0.5, 0.25, 1.0, 1.0, 0.3333333333333333, 0.25, 0.5, 0.2, 1.0, 0.125, 0.3333333333333333, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.3333333333333333, 0.3333333333333333, 1.0, 0.25, 0.5, 1.0, 0.5, 0.3333333333333333, 0.25, 1.0, 1.0, 1.0, 0.25, 1.0, 0.5, 1.0, 1.0, 1.0, 0.25, 0.3333333333333333, 1.0, 0.5, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.14285714285714285, 0.5, 0.5, 1.0, 0.5, 1.0, 0.5, 0.5, 0.5, 0.2, 1.0, 0.3333333333333333, 0.16666666666666666, 0.25, 0.5, 0.3333333333333333, 1.0, 0.25, 0.3333333333333333, 0.0, 1.0, 0.125, 0.5, 0.25, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.16666666666666666, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.5, 0.5, 0.3333333333333333, 0.16666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.2, 0.2, 0.2, 0.3333333333333333, 1.0, 0.2, 1.0, 0.0, 0.14285714285714285, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.25, 1.0, 1.0, 0.25, 0.25, 0.0, 0.5, 0.2]\n",
      "    Retriever Name  Hit Rate       MRR\n",
      "0  Retriever top_8  0.924658  0.600587\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]\n",
      "[1.0, 1.0, 0.16666666666666666, 0.3333333333333333, 1.0, 0.5, 0.5, 1.0, 0.16666666666666666, 1.0, 0.25, 0.5, 0.25, 1.0, 1.0, 0.3333333333333333, 0.25, 0.5, 0.2, 1.0, 0.125, 0.3333333333333333, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.3333333333333333, 0.3333333333333333, 1.0, 0.25, 0.5, 1.0, 0.5, 0.3333333333333333, 0.25, 1.0, 1.0, 1.0, 0.25, 1.0, 0.5, 1.0, 1.0, 1.0, 0.25, 0.3333333333333333, 1.0, 0.5, 0.0, 0.1, 0.5, 1.0, 1.0, 1.0, 1.0, 0.14285714285714285, 0.5, 0.5, 1.0, 0.5, 1.0, 0.5, 0.5, 0.5, 0.2, 1.0, 0.3333333333333333, 0.16666666666666666, 0.25, 0.5, 0.3333333333333333, 1.0, 0.25, 0.3333333333333333, 0.0, 1.0, 0.125, 0.5, 0.25, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.16666666666666666, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.5, 0.5, 0.3333333333333333, 0.16666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.2, 0.2, 0.2, 0.3333333333333333, 1.0, 0.2, 1.0, 0.0, 0.14285714285714285, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.25, 1.0, 1.0, 0.25, 0.25, 0.0, 0.5, 0.2]\n",
      "     Retriever Name  Hit Rate       MRR\n",
      "0  Retriever top_10  0.931507  0.601272\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "\n",
    "# we can evaluate the retrievers with different top_k values.\n",
    "\n",
    "for i in range(2, 10 + 1, 2):\n",
    "    retriever = vector_index.as_retriever(similarity_top_k= i)\n",
    "    retriever_evaluator= RetrieverEvaluator.from_metric_names(['mrr', 'hit_rate'], retriever= retriever)\n",
    "\n",
    "    eval_results= await retriever_evaluator.aevaluate_dataset(qc_dataset)\n",
    "    print(display_results_retriever(f\"Retriever top_{i}\", eval_results, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "- Notice how hit rate increase as our top k increase as well, which is expected. We're increasing the probability of the correct answer being included in the returned set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation for Relevancy and Faithfulness metrics\n",
    "\n",
    "- **Relevancy**: evaluates whether the retrieved context and answer are relevant to the query.\n",
    "- **Faithfulness**: evaluates if the answer is faithful to the retrieved contexts or, in other words, whether there's hallucination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11136/1470468060.py:16: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context_gpt4 = ServiceContext.from_defaults(llm=llm_gpt4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_2 faithfulness_score: 1.0\n",
      "top_2 relevancy_score: 1.0\n",
      "top_4 faithfulness_score: 1.0\n",
      "top_4 relevancy_score: 1.0\n",
      "top_6 faithfulness_score: 1.0\n",
      "top_6 relevancy_score: 1.0\n",
      "top_8 faithfulness_score: 1.0\n",
      "top_8 relevancy_score: 1.0\n",
      "top_10 faithfulness_score: 1.0\n",
      "top_10 relevancy_score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import (\n",
    "    RelevancyEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    "    BatchEvalRunner,\n",
    ")\n",
    "\n",
    "for i in range(2, 10 + 1, 2):\n",
    "    # set faithfulness and relevancy evaluators\n",
    "    query_engine = vector_index.as_query_engine(similarity_top_k=i)\n",
    "\n",
    "    # while we use gpt3.5-turbo to answer questions\n",
    "    # we can use gpt4 to evaluate the answers\n",
    "    llm_gpt4 = OpenAI(temperature=0, model=\"gpt-4-0125-preview\")\n",
    "\n",
    "    service_context_gpt4 = ServiceContext.from_defaults(llm=llm_gpt4)\n",
    "\n",
    "    faithfulness_evaluator = FaithfulnessEvaluator(service_context=service_context_gpt4)\n",
    "    relevancy_evaluator = RelevancyEvaluator(service_context=service_context_gpt4)\n",
    "\n",
    "    # Run evaluation\n",
    "    queries = list(qc_dataset.queries.values())\n",
    "    batch_eval_queries = queries[:20]\n",
    "\n",
    "    runner = BatchEvalRunner(\n",
    "        {\"faithfulness\": faithfulness_evaluator, \"relevancy\": relevancy_evaluator},\n",
    "        workers=8,\n",
    "    )\n",
    "\n",
    "    eval_results = await runner.aevaluate_queries(\n",
    "        query_engine, queries=batch_eval_queries\n",
    "    )\n",
    "\n",
    "    faithfulness_score = sum(\n",
    "        result.passing for result in eval_results[\"faithfulness\"]\n",
    "    ) / len(eval_results[\"faithfulness\"])\n",
    "    print(f\"top_{i} faithfulness_score: {faithfulness_score}\")\n",
    "\n",
    "    relevancy_score = sum(\n",
    "        result.passing for result in eval_results[\"faithfulness\"]\n",
    "    ) / len(eval_results[\"relevancy\"])\n",
    "    print(f\"top_{i} relevancy_score: {relevancy_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "\n",
    "Here above we are getting score=1.0 for each top_i as we are using gpt-4 and the data we are using for retrieval is not too complicated. But, for a larger dataset and vector store these scores may vary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default eval prompt template\n",
    "\n",
    "```py\n",
    "DEFAULT_EVAL_TEMPLATE = PromptTemplate(\n",
    "    \"Your task is to evaluate if the response for the query \\\n",
    "    is in line with the context information provided.\\n\"\n",
    "    \"You have two options to answer. Either YES/ NO.\\n\"\n",
    "    \"Answer - YES, if the response for the query \\\n",
    "    is in line with context information otherwise NO.\\n\"\n",
    "    \"Query and Response: \\n {query_str}\\n\"\n",
    "    \"Context: \\n {context_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing the embedding model\n",
    "\n",
    "- Checkout huggingface leaderboard for best embeddings: https://huggingface.co/spaces/mteb/leaderboard\n",
    "- Now that we have the baseline evaluation score, we can start changing some modules of our LlamaIndex RAG pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohere Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in ./data/paul_graham/deep_lake_db_1 already exists, loading from the storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10789/3677884839.py:17: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm,)\n",
      "Generating embeddings:   0%|          | 0/61 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "CohereAPIError",
     "evalue": "invalid api token",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCohereAPIError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m service_context \u001b[38;5;241m=\u001b[39m ServiceContext\u001b[38;5;241m.\u001b[39mfrom_defaults(embed_model\u001b[38;5;241m=\u001b[39membed_model, llm\u001b[38;5;241m=\u001b[39mllm,)\n\u001b[1;32m     18\u001b[0m storage_context \u001b[38;5;241m=\u001b[39m StorageContext\u001b[38;5;241m.\u001b[39mfrom_defaults(vector_store\u001b[38;5;241m=\u001b[39mvector_store)\n\u001b[0;32m---> 19\u001b[0m vector_index \u001b[38;5;241m=\u001b[39m \u001b[43mVectorStoreIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/llama_index/core/indices/vector_store/base.py:74\u001b[0m, in \u001b[0;36mVectorStoreIndex.__init__\u001b[0;34m(self, nodes, use_async, store_nodes_override, embed_model, insert_batch_size, objects, index_struct, storage_context, callback_manager, transformations, show_progress, service_context, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     68\u001b[0m     resolve_embed_model(embed_model, callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embed_model\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m embed_model_from_settings_or_context(Settings, service_context)\n\u001b[1;32m     71\u001b[0m )\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size \u001b[38;5;241m=\u001b[39m insert_batch_size\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/llama_index/core/indices/base.py:94\u001b[0m, in \u001b[0;36mBaseIndex.__init__\u001b[0;34m(self, nodes, objects, index_struct, storage_context, callback_manager, transformations, show_progress, service_context, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_struct \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m---> 94\u001b[0m     index_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct \u001b[38;5;241m=\u001b[39m index_struct\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage_context\u001b[38;5;241m.\u001b[39mindex_store\u001b[38;5;241m.\u001b[39madd_index_struct(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct)\n",
      "File \u001b[0;32m~/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/llama_index/core/indices/vector_store/base.py:307\u001b[0m, in \u001b[0;36mVectorStoreIndex.build_index_from_nodes\u001b[0;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    300\u001b[0m     node\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39mMetadataMode\u001b[38;5;241m.\u001b[39mEMBED) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[1;32m    301\u001b[0m ):\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot build index from nodes with no content. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure all nodes have content.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m     )\n\u001b[0;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minsert_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/llama_index/core/indices/vector_store/base.py:279\u001b[0m, in \u001b[0;36mVectorStoreIndex._build_index_from_nodes\u001b[0;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m     run_async_tasks(tasks)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_nodes_to_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minsert_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index_struct\n",
      "File \u001b[0;32m~/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/llama_index/core/indices/vector_store/base.py:232\u001b[0m, in \u001b[0;36mVectorStoreIndex._add_nodes_to_index\u001b[0;34m(self, index_struct, nodes, show_progress, **insert_kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nodes_batch \u001b[38;5;129;01min\u001b[39;00m iter_batch(nodes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size):\n\u001b[0;32m--> 232\u001b[0m     nodes_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_node_with_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     new_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_store\u001b[38;5;241m.\u001b[39madd(nodes_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minsert_kwargs)\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_store\u001b[38;5;241m.\u001b[39mstores_text \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_nodes_override:\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;66;03m# NOTE: if the vector store doesn't store text,\u001b[39;00m\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;66;03m# we need to add the nodes to the index struct and document store\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/llama_index/core/indices/vector_store/base.py:140\u001b[0m, in \u001b[0;36mVectorStoreIndex._get_node_with_embedding\u001b[0;34m(self, nodes, show_progress)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_node_with_embedding\u001b[39m(\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    131\u001b[0m     nodes: Sequence[BaseNode],\n\u001b[1;32m    132\u001b[0m     show_progress: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    133\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[BaseNode]:\n\u001b[1;32m    134\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get tuples of id, node, and embedding.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    Allows us to store these nodes in a vector store.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m    Embeddings are called in batches.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     id_to_embed_map \u001b[38;5;241m=\u001b[39m \u001b[43membed_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes:\n",
      "File \u001b[0;32m~/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/llama_index/core/indices/utils.py:138\u001b[0m, in \u001b[0;36membed_nodes\u001b[0;34m(nodes, embed_model, show_progress)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m         id_to_embed_map[node\u001b[38;5;241m.\u001b[39mnode_id] \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39membedding\n\u001b[0;32m--> 138\u001b[0m new_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membed_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text_embedding_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts_to_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m new_id, text_embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ids_to_embed, new_embeddings):\n\u001b[1;32m    143\u001b[0m     id_to_embed_map[new_id] \u001b[38;5;241m=\u001b[39m text_embedding\n",
      "File \u001b[0;32m~/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/llama_index/core/base/embeddings/base.py:255\u001b[0m, in \u001b[0;36mBaseEmbedding.get_text_embedding_batch\u001b[0;34m(self, texts, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cur_batch) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_batch_size:\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# flush\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    252\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mEMBEDDING,\n\u001b[1;32m    253\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mSERIALIZED: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dict()},\n\u001b[1;32m    254\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[0;32m--> 255\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_text_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m         result_embeddings\u001b[38;5;241m.\u001b[39mextend(embeddings)\n\u001b[1;32m    257\u001b[0m         event\u001b[38;5;241m.\u001b[39mon_end(\n\u001b[1;32m    258\u001b[0m             payload\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    259\u001b[0m                 EventPayload\u001b[38;5;241m.\u001b[39mCHUNKS: cur_batch,\n\u001b[1;32m    260\u001b[0m                 EventPayload\u001b[38;5;241m.\u001b[39mEMBEDDINGS: embeddings,\n\u001b[1;32m    261\u001b[0m             },\n\u001b[1;32m    262\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/llama_index/embeddings/cohere/base.py:179\u001b[0m, in \u001b[0;36mCohereEmbedding._get_text_embeddings\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_text_embeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m    178\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get text embeddings.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msearch_document\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/llama_index/embeddings/cohere/base.py:149\u001b[0m, in \u001b[0;36mCohereEmbedding._embed\u001b[0;34m(self, texts, input_type)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Embed sentences using Cohere.\"\"\"\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m     CAMN\u001b[38;5;241m.\u001b[39mENGLISH_V3,\n\u001b[1;32m    145\u001b[0m     CAMN\u001b[38;5;241m.\u001b[39mENGLISH_LIGHT_V3,\n\u001b[1;32m    146\u001b[0m     CAMN\u001b[38;5;241m.\u001b[39mMULTILINGUAL_V3,\n\u001b[1;32m    147\u001b[0m     CAMN\u001b[38;5;241m.\u001b[39mMULTILINGUAL_LIGHT_V3,\n\u001b[1;32m    148\u001b[0m ]:\n\u001b[0;32m--> 149\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcohere_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minput_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39membeddings\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    156\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcohere_client\u001b[38;5;241m.\u001b[39membed(\n\u001b[1;32m    157\u001b[0m         texts\u001b[38;5;241m=\u001b[39mtexts, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncate\n\u001b[1;32m    158\u001b[0m     )\u001b[38;5;241m.\u001b[39membeddings\n",
      "File \u001b[0;32m~/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/cohere/client.py:425\u001b[0m, in \u001b[0;36mClient.embed\u001b[0;34m(self, texts, model, truncate, input_type, embedding_types)\u001b[0m\n\u001b[1;32m    414\u001b[0m     json_bodys\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    415\u001b[0m         {\n\u001b[1;32m    416\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m         }\n\u001b[1;32m    422\u001b[0m     )\n\u001b[1;32m    424\u001b[0m meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m json_body: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(cohere\u001b[38;5;241m.\u001b[39mEMBED_URL, json\u001b[38;5;241m=\u001b[39mjson_body), json_bodys):\n\u001b[1;32m    426\u001b[0m     embedding_responses\u001b[38;5;241m.\u001b[39madd_response(result)\n\u001b[1;32m    427\u001b[0m     meta \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m meta \u001b[38;5;28;01melse\u001b[39;00m meta\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/cohere/client.py:425\u001b[0m, in \u001b[0;36mClient.embed.<locals>.<lambda>\u001b[0;34m(json_body)\u001b[0m\n\u001b[1;32m    414\u001b[0m     json_bodys\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    415\u001b[0m         {\n\u001b[1;32m    416\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m         }\n\u001b[1;32m    422\u001b[0m     )\n\u001b[1;32m    424\u001b[0m meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m json_body: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcohere\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEMBED_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson_body\u001b[49m\u001b[43m)\u001b[49m, json_bodys):\n\u001b[1;32m    426\u001b[0m     embedding_responses\u001b[38;5;241m.\u001b[39madd_response(result)\n\u001b[1;32m    427\u001b[0m     meta \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m meta \u001b[38;5;28;01melse\u001b[39;00m meta\n",
      "File \u001b[0;32m~/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/cohere/client.py:933\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, endpoint, json, files, method, stream, params)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m jsonlib\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mJSONDecodeError:  \u001b[38;5;66;03m# CohereAPIError will capture status\u001b[39;00m\n\u001b[1;32m    931\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CohereAPIError\u001b[38;5;241m.\u001b[39mfrom_response(response, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to decode json body: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 933\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m json_response\n",
      "File \u001b[0;32m~/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/cohere/client.py:875\u001b[0m, in \u001b[0;36mClient._check_response\u001b[0;34m(self, json_response, headers, status_code)\u001b[0m\n\u001b[1;32m    873\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX-API-Warning\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m json_response:  \u001b[38;5;66;03m# has errors\u001b[39;00m\n\u001b[0;32m--> 875\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CohereAPIError(\n\u001b[1;32m    876\u001b[0m         message\u001b[38;5;241m=\u001b[39mjson_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    877\u001b[0m         http_status\u001b[38;5;241m=\u001b[39mstatus_code,\n\u001b[1;32m    878\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    879\u001b[0m     )\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m400\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m status_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m:\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CohereAPIError(\n\u001b[1;32m    882\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected client error (status \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    883\u001b[0m         http_status\u001b[38;5;241m=\u001b[39mstatus_code,\n\u001b[1;32m    884\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    885\u001b[0m     )\n",
      "\u001b[0;31mCohereAPIError\u001b[0m: invalid api token"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_index.core import VectorStoreIndex, ServiceContext, StorageContext\n",
    "from llama_index.embeddings.cohere import CohereEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Create another local DeepLakeVectorStore to store the embeddings\n",
    "dataset_path = \"./data/paul_graham/deep_lake_db_1\"\n",
    "vector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=False)\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "embed_model = CohereEmbedding(\n",
    "    cohere_api_key=os.getenv('COHERE_API_KEY'),\n",
    "    model_name=\"embed-english-v3.0\",\n",
    "    input_type=\"search_document\",\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm,)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "vector_index = VectorStoreIndex(nodes, service_context=service_context, storage_context=storage_context, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval using cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "CohereAPIError",
     "evalue": "You are using a Trial key, which is limited to 100 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCohereAPIError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m retriever \u001b[38;5;241m=\u001b[39m vector_index\u001b[38;5;241m.\u001b[39mas_retriever(similarity_top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, embed_model\u001b[38;5;241m=\u001b[39m embed_model)\n\u001b[1;32m      5\u001b[0m retriever_evaluator \u001b[38;5;241m=\u001b[39m RetrieverEvaluator\u001b[38;5;241m.\u001b[39mfrom_metric_names([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmrr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhit_rate\u001b[39m\u001b[38;5;124m'\u001b[39m], retriever\u001b[38;5;241m=\u001b[39m retriever)\n\u001b[0;32m----> 6\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m retriever_evaluator\u001b[38;5;241m.\u001b[39maevaluate_dataset(qc_dataset, show_progres\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(display_results_retriever(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetriever_cohere_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m, eval_results))\n",
      "File \u001b[0;32m~/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/llama_index/core/evaluation/retrieval/base.py:197\u001b[0m, in \u001b[0;36mBaseRetrievalEvaluator.aevaluate_dataset\u001b[0;34m(self, dataset, workers, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     eval_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m tqdm_asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mresponse_jobs)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     eval_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mresponse_jobs)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m eval_results\n",
      "File \u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py:304\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/llama_index/core/evaluation/retrieval/base.py:185\u001b[0m, in \u001b[0;36mBaseRetrievalEvaluator.aevaluate_dataset.<locals>.eval_worker\u001b[0;34m(query, expected_ids, mode)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_worker\u001b[39m(\n\u001b[1;32m    182\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m, expected_ids: List[\u001b[38;5;28mstr\u001b[39m], mode: RetrievalEvalMode\n\u001b[1;32m    183\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RetrievalEvalResult:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m semaphore:\n\u001b[0;32m--> 185\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maevaluate(query, expected_ids\u001b[38;5;241m=\u001b[39mexpected_ids, mode\u001b[38;5;241m=\u001b[39mmode)\n",
      "File \u001b[0;32m~/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/llama_index/core/evaluation/retrieval/base.py:151\u001b[0m, in \u001b[0;36mBaseRetrievalEvaluator.aevaluate\u001b[0;34m(self, query, expected_ids, expected_texts, mode, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maevaluate\u001b[39m(\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    139\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    144\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RetrievalEvalResult:\n\u001b[1;32m    145\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run evaluation with query string, retrieved contexts,\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    and generated response string.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m    Subclasses can override this method to provide custom evaluation logic and\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m    take in additional arguments.\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m     retrieved_ids, retrieved_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aget_retrieved_ids_and_texts(\n\u001b[1;32m    152\u001b[0m         query, mode\n\u001b[1;32m    153\u001b[0m     )\n\u001b[1;32m    154\u001b[0m     metric_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/llama_index/core/evaluation/retrieval/evaluator.py:56\u001b[0m, in \u001b[0;36mRetrieverEvaluator._aget_retrieved_ids_and_texts\u001b[0;34m(self, query, mode)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_aget_retrieved_ids_and_texts\u001b[39m(\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, mode: RetrievalEvalMode \u001b[38;5;241m=\u001b[39m RetrievalEvalMode\u001b[38;5;241m.\u001b[39mTEXT\n\u001b[1;32m     54\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], List[\u001b[38;5;28mstr\u001b[39m]]:\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get retrieved ids and texts, potentially applying a post-processor.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     retrieved_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretriever\u001b[38;5;241m.\u001b[39maretrieve(query)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_postprocessors:\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m node_postprocessor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_postprocessors:\n",
      "File \u001b[0;32m~/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/llama_index/core/base/base_retriever.py:249\u001b[0m, in \u001b[0;36mBaseRetriever.aretrieve\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mas_trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    246\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mRETRIEVE,\n\u001b[1;32m    247\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str},\n\u001b[1;32m    248\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m retrieve_event:\n\u001b[0;32m--> 249\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aretrieve(query_bundle)\n\u001b[1;32m    250\u001b[0m         nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ahandle_recursive_retrieval(query_bundle, nodes)\n\u001b[1;32m    251\u001b[0m         retrieve_event\u001b[38;5;241m.\u001b[39mon_end(\n\u001b[1;32m    252\u001b[0m             payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mNODES: nodes},\n\u001b[1;32m    253\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/llama_index/core/indices/vector_store/retrievers/retriever.py:101\u001b[0m, in \u001b[0;36mVectorIndexRetriever._aretrieve\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_bundle\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(query_bundle\u001b[38;5;241m.\u001b[39membedding_strs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     99\u001b[0m         embed_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model\n\u001b[1;32m    100\u001b[0m         query_bundle\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 101\u001b[0m             \u001b[38;5;28;01mawait\u001b[39;00m embed_model\u001b[38;5;241m.\u001b[39maget_agg_embedding_from_queries(\n\u001b[1;32m    102\u001b[0m                 query_bundle\u001b[38;5;241m.\u001b[39membedding_strs\n\u001b[1;32m    103\u001b[0m             )\n\u001b[1;32m    104\u001b[0m         )\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aget_nodes_with_embeddings(query_bundle)\n",
      "File \u001b[0;32m~/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/llama_index/core/base/embeddings/base.py:151\u001b[0m, in \u001b[0;36mBaseEmbedding.aget_agg_embedding_from_queries\u001b[0;34m(self, queries, agg_fn)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maget_agg_embedding_from_queries\u001b[39m(\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    147\u001b[0m     queries: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    148\u001b[0m     agg_fn: Optional[Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Embedding]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    149\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Embedding:\n\u001b[1;32m    150\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Async get aggregated embedding from multiple queries.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m     query_embeddings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maget_query_embedding(query) \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries]\n\u001b[1;32m    152\u001b[0m     agg_fn \u001b[38;5;241m=\u001b[39m agg_fn \u001b[38;5;129;01mor\u001b[39;00m mean_agg\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m agg_fn(query_embeddings)\n",
      "File \u001b[0;32m~/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/llama_index/core/base/embeddings/base.py:151\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maget_agg_embedding_from_queries\u001b[39m(\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    147\u001b[0m     queries: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    148\u001b[0m     agg_fn: Optional[Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Embedding]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    149\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Embedding:\n\u001b[1;32m    150\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Async get aggregated embedding from multiple queries.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m     query_embeddings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maget_query_embedding(query) \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries]\n\u001b[1;32m    152\u001b[0m     agg_fn \u001b[38;5;241m=\u001b[39m agg_fn \u001b[38;5;129;01mor\u001b[39;00m mean_agg\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m agg_fn(query_embeddings)\n",
      "File \u001b[0;32m~/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/llama_index/core/base/embeddings/base.py:125\u001b[0m, in \u001b[0;36mBaseEmbedding.aget_query_embedding\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get query embedding.\"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    123\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mEMBEDDING, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mSERIALIZED: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dict()}\n\u001b[1;32m    124\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[0;32m--> 125\u001b[0m     query_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aget_query_embedding(query)\n\u001b[1;32m    127\u001b[0m     event\u001b[38;5;241m.\u001b[39mon_end(\n\u001b[1;32m    128\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    129\u001b[0m             EventPayload\u001b[38;5;241m.\u001b[39mCHUNKS: [query],\n\u001b[1;32m    130\u001b[0m             EventPayload\u001b[38;5;241m.\u001b[39mEMBEDDINGS: [query_embedding],\n\u001b[1;32m    131\u001b[0m         },\n\u001b[1;32m    132\u001b[0m     )\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query_embedding\n",
      "File \u001b[0;32m~/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/llama_index/embeddings/cohere/base.py:167\u001b[0m, in \u001b[0;36mCohereEmbedding._aget_query_embedding\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_aget_query_embedding\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get query embedding async. For query embeddings, input_type='search_query'.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_query_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/llama_index/embeddings/cohere/base.py:163\u001b[0m, in \u001b[0;36mCohereEmbedding._get_query_embedding\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_query_embedding\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get query embedding. For query embeddings, input_type='search_query'.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msearch_query\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/llama_index/embeddings/cohere/base.py:149\u001b[0m, in \u001b[0;36mCohereEmbedding._embed\u001b[0;34m(self, texts, input_type)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Embed sentences using Cohere.\"\"\"\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m     CAMN\u001b[38;5;241m.\u001b[39mENGLISH_V3,\n\u001b[1;32m    145\u001b[0m     CAMN\u001b[38;5;241m.\u001b[39mENGLISH_LIGHT_V3,\n\u001b[1;32m    146\u001b[0m     CAMN\u001b[38;5;241m.\u001b[39mMULTILINGUAL_V3,\n\u001b[1;32m    147\u001b[0m     CAMN\u001b[38;5;241m.\u001b[39mMULTILINGUAL_LIGHT_V3,\n\u001b[1;32m    148\u001b[0m ]:\n\u001b[0;32m--> 149\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcohere_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minput_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39membeddings\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    156\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcohere_client\u001b[38;5;241m.\u001b[39membed(\n\u001b[1;32m    157\u001b[0m         texts\u001b[38;5;241m=\u001b[39mtexts, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncate\n\u001b[1;32m    158\u001b[0m     )\u001b[38;5;241m.\u001b[39membeddings\n",
      "File \u001b[0;32m~/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/cohere/client.py:425\u001b[0m, in \u001b[0;36mClient.embed\u001b[0;34m(self, texts, model, truncate, input_type, embedding_types)\u001b[0m\n\u001b[1;32m    414\u001b[0m     json_bodys\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    415\u001b[0m         {\n\u001b[1;32m    416\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m         }\n\u001b[1;32m    422\u001b[0m     )\n\u001b[1;32m    424\u001b[0m meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m json_body: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(cohere\u001b[38;5;241m.\u001b[39mEMBED_URL, json\u001b[38;5;241m=\u001b[39mjson_body), json_bodys):\n\u001b[1;32m    426\u001b[0m     embedding_responses\u001b[38;5;241m.\u001b[39madd_response(result)\n\u001b[1;32m    427\u001b[0m     meta \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m meta \u001b[38;5;28;01melse\u001b[39;00m meta\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/cohere/client.py:425\u001b[0m, in \u001b[0;36mClient.embed.<locals>.<lambda>\u001b[0;34m(json_body)\u001b[0m\n\u001b[1;32m    414\u001b[0m     json_bodys\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    415\u001b[0m         {\n\u001b[1;32m    416\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m         }\n\u001b[1;32m    422\u001b[0m     )\n\u001b[1;32m    424\u001b[0m meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executor\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m json_body: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcohere\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEMBED_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson_body\u001b[49m\u001b[43m)\u001b[49m, json_bodys):\n\u001b[1;32m    426\u001b[0m     embedding_responses\u001b[38;5;241m.\u001b[39madd_response(result)\n\u001b[1;32m    427\u001b[0m     meta \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m meta \u001b[38;5;28;01melse\u001b[39;00m meta\n",
      "File \u001b[0;32m~/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/cohere/client.py:933\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, endpoint, json, files, method, stream, params)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m jsonlib\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mJSONDecodeError:  \u001b[38;5;66;03m# CohereAPIError will capture status\u001b[39;00m\n\u001b[1;32m    931\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CohereAPIError\u001b[38;5;241m.\u001b[39mfrom_response(response, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to decode json body: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 933\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m json_response\n",
      "File \u001b[0;32m~/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/cohere/client.py:875\u001b[0m, in \u001b[0;36mClient._check_response\u001b[0;34m(self, json_response, headers, status_code)\u001b[0m\n\u001b[1;32m    873\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX-API-Warning\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m json_response:  \u001b[38;5;66;03m# has errors\u001b[39;00m\n\u001b[0;32m--> 875\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CohereAPIError(\n\u001b[1;32m    876\u001b[0m         message\u001b[38;5;241m=\u001b[39mjson_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    877\u001b[0m         http_status\u001b[38;5;241m=\u001b[39mstatus_code,\n\u001b[1;32m    878\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    879\u001b[0m     )\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m400\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m status_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m:\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CohereAPIError(\n\u001b[1;32m    882\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected client error (status \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    883\u001b[0m         http_status\u001b[38;5;241m=\u001b[39mstatus_code,\n\u001b[1;32m    884\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    885\u001b[0m     )\n",
      "\u001b[0;31mCohereAPIError\u001b[0m: You are using a Trial key, which is limited to 100 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.com/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "\n",
    "# embed_model.input_type = \"search_query\"\n",
    "retriever = vector_index.as_retriever(similarity_top_k=10, embed_model= embed_model)\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(['mrr', 'hit_rate'], retriever= retriever)\n",
    "eval_results = await retriever_evaluator.aevaluate_dataset(qc_dataset, show_progres= True, workers=1)\n",
    "print(display_results_retriever(f\"Retriever_cohere_embeds\", eval_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reranker\n",
    "\n",
    "- cross-encoder/ms-marco-MiniLM-L-6-v2 from the Hugging Face hub.\n",
    "- LlamaIndex’s LLMRerank \n",
    "- Cohere’s CohereRerank. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Retriever Name  Hit Rate       MRR\n",
      "0  retriever with added reranker  0.931507  0.601272\n",
      "====================\n",
      "                  Retriever Name  Hit Rate       MRR\n",
      "0  retriever with added reranker  0.931507  0.601272\n",
      "====================\n",
      "                  Retriever Name  Hit Rate       MRR\n",
      "0  retriever with added reranker  0.931507  0.601272\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank, LLMRerank\n",
    "from llama_index.core.evaluation import RetrieverEvaluator\n",
    "\n",
    "st_reranker = SentenceTransformerRerank(\n",
    "    top_n=5, model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    ")\n",
    "\n",
    "llm_reranker = LLMRerank(choice_batch_size=4, top_n=5)\n",
    "embed_model = OpenAIEmbedding()\n",
    "cohere_rerank = CohereRerank(api_key=os.getenv(\"COHERE_API_KEY\"), top_n=10)\n",
    "\n",
    "for reranker in [st_reranker, llm_reranker, cohere_rerank]:\n",
    "    retriever_with_reranker = vector_index.as_retriever(\n",
    "        similarity_top_k=10, postprocessor=reranker, embed_model=embed_model\n",
    "    )\n",
    "\n",
    "    retriever_evaluator_1 = RetrieverEvaluator.from_metric_names(\n",
    "        [\"mrr\", \"hit_rate\"], retriever=retriever_with_reranker\n",
    "    )\n",
    "\n",
    "    eval_results1 = await retriever_evaluator_1.aevaluate_dataset(qc_dataset)\n",
    "    print(display_results_retriever(f\"retriever with added reranker\", eval_results1))\n",
    "    print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Employing activeloop's deep memory\n",
    "\n",
    "- This will train a tiny neural network layer to match user queries with relevant data from a corpus.\n",
    "- It can improve the retrieval accuracy by upto 27%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146\n"
     ]
    }
   ],
   "source": [
    "def create_query_relevance(qa_dataset):\n",
    "    \"\"\"Fun for converting llama index dataset to correct format for deep memeory training\"\"\"\n",
    "\n",
    "    queries= [text for _, text in qa_dataset.queries.items()]\n",
    "\n",
    "    relevant_docs = qa_dataset.relevant_docs\n",
    "    relevance = [[(relevant_docs[doc][0], 1)]  for doc in relevant_docs]\n",
    "    return queries, relevance\n",
    "\n",
    "train_queries, train_relevance = create_query_relevance(qc_dataset)\n",
    "print(len(train_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying dataset: 96%|█████████▋| 27/28 [00:31<00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/akshatunsubscribe/optimization_paul_graham\n",
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying dataset: 96%|█████████▋| 27/28 [00:48<00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/akshatunsubscribe/optimization_paul_graham_managed\n",
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset(path='hub://akshatunsubscribe/optimization_paul_graham_managed', tensors=['embedding', 'id', 'metadata', 'text'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import deeplake\n",
    "local = \"./data/paul_graham/deep_lake_db\"\n",
    "username = \"akshatunsubscribe\"\n",
    "\n",
    "hub_path = f\"hub://{username}/optimization_paul_graham\"\n",
    "hub_managed_path = f\"hub://{username}/optimization_paul_graham_managed\"\n",
    "\n",
    "# First upload our local vector store\n",
    "deeplake.deepcopy(local, hub_path, overwrite=True)\n",
    "# Create a managed vector store\n",
    "deeplake.deepcopy(hub_path, hub_managed_path, overwrite=True, runtime={\"tensor_db\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset from deep lake cloud storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://akshatunsubscribe/optimization_paul_graham_managed already exists, loading from the storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15773/3517154835.py:18: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_index.core import VectorStoreIndex, ServiceContext, StorageContext\n",
    "from llama_index.vector_stores.deeplake import DeepLakeVectorStore\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "embed_model = OpenAIEmbedding()\n",
    "\n",
    "vector_store = DeepLakeVectorStore(\n",
    "    dataset_path=hub_managed_path,\n",
    "    overwrite=False,\n",
    "    runtime={\"tensor_db\": True},\n",
    "    read_only=True,\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    embed_model=embed_model,\n",
    "    llm=llm,\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "vector_index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store,\n",
    "    service_context=service_context,\n",
    "    storage_context=storage_context,\n",
    "    use_async=False,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DeepMemory training job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshat/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data for deepmemory:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 146 embeddings in 1 batches of size 146:: 100%|██████████| 1/1 [00:37<00:00, 37.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepMemory training job started. Job ID: 660126675787feddb98239c6\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "job_id = vector_store._vectorstore.deep_memory.train(\n",
    "    queries=train_queries,\n",
    "    relevance=train_relevance,\n",
    "    embedding_function=embeddings.embed_documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/akshatunsubscribe/optimization_paul_graham_managed\n",
      "--------------------------------------------------------------\n",
      "|                  660126675787feddb98239c6                  |\n",
      "--------------------------------------------------------------\n",
      "| status                     | completed                     |\n",
      "--------------------------------------------------------------\n",
      "| progress                   | eta: 0.5 seconds              |\n",
      "|                            | recall@10: 86.21% (+10.34%)   |\n",
      "--------------------------------------------------------------\n",
      "| results                    | recall@10: 86.21% (+10.34%)   |\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vector_store._vectorstore.deep_memory.status(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate deep memory new embeddings retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import generate_question_context_pairs\n",
    "\n",
    "if not os.path.exists(\"./data/test_dataset.json\"):\n",
    "    # Generate dataset\n",
    "    test_dataset = generate_question_context_pairs(\n",
    "        nodes[:20],\n",
    "        llm= llm,\n",
    "        num_questions_per_chunk= 1\n",
    "    )\n",
    "\n",
    "    test_dataset.save_json(\"./data/test_dataset.json\")\n",
    "\n",
    "else:\n",
    "    # Load from local if already exists\n",
    "    from llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n",
    "\n",
    "\n",
    "    test_dataset = EmbeddingQAFinetuneDataset.from_json(\"./data/test_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start creating dataset for deep memory\n",
    "test_queries, test_relevance= create_query_relevance(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate test set for @recall\n",
    "\n",
    "- **Recall** is calculates as  `TP/(TP + FN)` where TP= True Pos; FN= False Neg.\n",
    "\n",
    "- In LLM's we @recall formula is (# of relevant items retrieved)/(Total # of relevant items in dataset).\n",
    "\n",
    "- Compared to hit rate recall is about the system's attention to detail in retrieving all the relevant items as we consider all the retrieved items relevancy, and hit rate is ensuring that each query retrieves something relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding queries took 1.73 seconds\n",
      "---- Evaluating without Deep Memory ---- \n",
      "Recall@1:\t  38.8%\n",
      "Recall@3:\t  79.6%\n",
      "Recall@5:\t  87.8%\n",
      "Recall@10:\t  93.9%\n",
      "Recall@50:\t  100.0%\n",
      "Recall@100:\t  100.0%\n",
      "---- Evaluating with Deep Memory ---- \n",
      "Recall@1:\t  49.0%\n",
      "Recall@3:\t  81.6%\n",
      "Recall@5:\t  89.8%\n",
      "Recall@10:\t  91.8%\n",
      "Recall@50:\t  100.0%\n",
      "Recall@100:\t  100.0%\n"
     ]
    }
   ],
   "source": [
    "recalls = vector_store._vectorstore.deep_memory.evaluate(\n",
    "    queries= test_queries,\n",
    "    relevance= test_relevance,\n",
    "    embedding_function= embeddings.embed_documents\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[1.0, 1.0, 0.5, 0.5, 1.0, 0.1, 1.0, 0.25, 0.5, 1.0, 0.0, 1.0, 0.5, 0.5, 0.2, 1.0, 0.5, 1.0, 0.14285714285714285, 0.5, 0.5, 1.0, 0.5, 0.1, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.3333333333333333, 1.0, 0.5, 1.0, 0.3333333333333333, 1.0, 0.5, 0.0, 1.0, 0.25, 0.5, 0.5, 0.5, 0.5, 0.2]\n",
      "      Retriever Name  Hit Rate       MRR\n",
      "0  Retriever Results  0.938776  0.610398\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "from llama_index.core.evaluation import (\n",
    "    RetrieverEvaluator,\n",
    ")\n",
    "\n",
    "base_retriever = vector_index.as_retriever(similarity_top_k=10)\n",
    "deep_memory_retriever = vector_index.as_retriever(\n",
    "    similarity_top_k=10, vector_store_kwargs={\"deep_memory\": True}\n",
    ")\n",
    "\n",
    "base_retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    [\"mrr\", \"hit_rate\"], retriever=base_retriever\n",
    ")\n",
    "eval_results = await base_retriever_evaluator.aevaluate_dataset(test_dataset)\n",
    "print(display_results_retriever(\"Retriever Results\", eval_results, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "\n",
    "- Looking at the below table we can see that deep memory indeed increases the MRR and Hit Rate.\n",
    "- The gains may not be too high as we are using a very small train/test dataset, using a bigger dataset can significantly increase the evaluation metrics.\n",
    "\n",
    "```text\n",
    "\n",
    "     Retriever Name               Hit Rate       MRR\n",
    "0  Retriever top_10               0.931507  0.601272\n",
    "====================\n",
    "                  Retriever Name  Hit Rate       MRR\n",
    "0  retriever with added reranker  0.931507  0.601272\n",
    "====================\n",
    "                  Retriever Name  Hit Rate       MRR\n",
    "0  retriever with added reranker  0.931507  0.601272\n",
    "====================\n",
    "                  Retriever Name  Hit Rate       MRR\n",
    "0  retriever with added reranker  0.931507  0.601272\n",
    "====================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resourses\n",
    "\n",
    "- [Hit Rate & MRR Blog](https://tamilselvan-subramanian.medium.com/how-hit-rate-and-mrr-measure-llm-retrievers-ai-simplified-series-7203ba2d4032#:~:text=Remember%3A-,Hit%20Rate%20tells%20you%20if%20the%20LLM%20found%20any%20relevant,the%20future%20of%20information%20access.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
