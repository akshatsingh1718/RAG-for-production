{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install deeplake langchain openai tiktoken llama-index\n",
    "! pip install -q llama-index-embeddings-openai llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 75042  100 75042    0     0  52257      0  0:00:01  0:00:01 --:--:-- 52221\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p 'data/paul_graham/'\n",
    "!curl 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -o 'data/paul_graham/paul_graham_essay.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .3 Create chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents: 1\n",
      "Number of nodes: 61 with the current chunk size of 512\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.core.readers import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
    "node_parser = SimpleNodeParser.from_defaults(chunk_size=512)\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "# By default, the node/chunks ids are set to random uuids. To ensure same id's per run, we manually set them.\n",
    "for idx, node in enumerate(nodes):\n",
    "    node.id_ = f\"node_{idx}\"\n",
    "\n",
    "print(f\"Number of Documents: {len(documents)}\")\n",
    "print(f\"Number of nodes: {len(nodes)} with the current chunk size of {node_parser.chunk_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Deep lake vector store\n",
    "\n",
    "In simple terms do the following steps:\n",
    "\n",
    "1. chunk the document you have into smaller chunks (already did in step 3). \n",
    "2. using embeddigns model create embedding for these chunks.\n",
    "3. Save these embeddigns to local vector store using deep lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29551/2218410595.py:14: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(\n",
      "/home/akshat/Documents/courses/ActiveloopProdRAG/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating embeddings: 100%|██████████| 61/61 [00:02<00:00, 20.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading data to deeplake dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61/61 [00:00<00:00, 234.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='./data/paul_graham/deep_lake_db', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype      shape      dtype  compression\n",
      "  -------    -------    -------    -------  ------- \n",
      "   text       text      (61, 1)      str     None   \n",
      " metadata     json      (61, 1)      str     None   \n",
      " embedding  embedding  (61, 1536)  float32   None   \n",
      "    id        text      (61, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, ServiceContext, StorageContext\n",
    "from llama_index.vector_stores.deeplake import DeepLakeVectorStore\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Create a DeepLakeVectorStore locally to store the vectors\n",
    "dataset_path = \"./data/paul_graham/deep_lake_db\"\n",
    "vector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=True)\n",
    "\n",
    "# LLM that will answer questions with the retrieved context\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "embed_model = OpenAIEmbedding()\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    embed_model=embed_model,\n",
    "    llm=llm,\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "vector_index = VectorStoreIndex(\n",
    "    nodes,\n",
    "    service_context=service_context,\n",
    "    storage_context=storage_context,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Upload local db to cloud\n",
    "\n",
    "Once the embeddings has been stored in your local database you can also upload that to deep lakes cloud infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying dataset: 96%|█████████▋| 27/28 [00:26<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/akshatsingh1718/optimization_paul_graham\n",
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying dataset: 96%|█████████▋| 27/28 [00:36<00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/akshatsingh1718/optimization_paul_graham_managed\n",
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset(path='hub://akshatsingh1718/optimization_paul_graham_managed', tensors=['embedding', 'id', 'metadata', 'text'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import deeplake\n",
    "local = \"./data/paul_graham/deep_lake_db\"\n",
    "\n",
    "username = \"akshatsingh1718\"\n",
    "\n",
    "hub_path = f\"hub://{username}/optimization_paul_graham\"\n",
    "hub_managed_path = f\"hub://{username}/optimization_paul_graham_managed\"\n",
    "\n",
    "# First upload our local vector store\n",
    "deeplake.deepcopy(local, hub_path, overwrite=True)\n",
    "# Create a managed vector store under a different name\n",
    "deeplake.deepcopy(hub_path, hub_managed_path, overwrite=True, runtime={\"tensor_db\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://akshatsingh1718/optimization_paul_graham_managed already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "# instantiate vector store with the managed database just created in deep lake cloud infrastructure\n",
    "db = DeepLakeVectorStore(dataset_path=hub_managed_path, overwrite=False, read_only=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fetch docs and ids from the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    }
   ],
   "source": [
    "docs = db._vectorstore.dataset.text.data(fetch_chunks=True, aslist=True)['value']\n",
    "ids = db._vectorstore.dataset.id.data(fetch_chunks=True, aslist=True)['value']\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate synthetic training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def generate_question(text):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo-1106\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a world class expert for generating questions based on provided context. \\\n",
    "                        You make sure the question can be answered by the text.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": text,\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except:\n",
    "        question_string = \"No question generated\"\n",
    "        return question_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def generate_queries(docs: list[str], ids: list[str], n: int):\n",
    "\n",
    "    questions = []\n",
    "    relevances = []\n",
    "    pbar = tqdm(total=n)\n",
    "    while len(questions) < n:\n",
    "        # 1. randomly draw a piece of text and relevance id\n",
    "        r = random.randint(0, len(docs) - 1)\n",
    "        text, label = docs[r], ids[r]\n",
    "\n",
    "        # 2. generate queries and assign and relevance id\n",
    "        generated_qs = [generate_question(text)]\n",
    "        if generated_qs == [\"No question generated\"]:\n",
    "            print(\"No question generated\")\n",
    "            continue\n",
    "\n",
    "        questions.extend(generated_qs) # questions += [ <gen_que1>, <gen_que2>, ... <gen_queN> ]\n",
    "        relevances.extend([[(label, 1)] for _ in generated_qs]) # [ (chunk_id, corpus_id), ... ]\n",
    "        pbar.update(len(generated_qs))\n",
    "\n",
    "    return questions[:n], relevances[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:37<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "What inspired the author to write another book on Lisp and what did the author imagine achieving from it?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "questions, relevances = generate_queries(docs, ids, n=40)\n",
    "print(len(questions)) #40\n",
    "print(questions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deep Memeory Training\n",
    "\n",
    "The deep memory model will train using (question, context) pair to better understand what type of question should return what type of contexts.\n",
    "\n",
    "The model will optimize the query and transforms them into space optimized for the specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "openai_embeddings = OpenAIEmbedding()\n",
    "\n",
    "job_id = db._vectorstore.deep_memory.train(\n",
    "    queries=questions,\n",
    "    relevance=relevances,\n",
    "    embedding_function=openai_embeddings.embed_documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.vectorstore.deep_memory.status(job_id=\"<Your_job_id>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inference using Deep memeory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "\n",
    "query = \"What are the main things Paul worked on before college?\"\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "embed_model = OpenAIEmbedding()\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm,)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "db = DeepLakeVectorStore(dataset_path=hub_managed_path, overwrite=False, read_only=True,)\n",
    "vector_index = VectorStoreIndex.from_vector_store(db, service_context=service_context, storage_context=storage_context, show_progress=True)\n",
    "\n",
    "query_engine = vector_index.as_query_engine(similarity_top_k=3, vector_store_kwargs={\"deep_memory\": True})\n",
    "response_vector = query_engine.query(query)\n",
    "print(response_vector.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluations (Deep memory vs Vanilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate validation queries\n",
    "validation_questions, validation_relevances = generate_queries(docs, ids, n=40)\n",
    "\n",
    "# Launch the evaluation function\n",
    "recalls = db._vectorstore.deep_memory.evaluate(\n",
    "    queries=validation_questions,\n",
    "    relevance=validation_relevances,\n",
    "    embedding_function=openai_embeddings.embed_documents,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
