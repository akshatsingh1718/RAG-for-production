{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production-Ready RAG Solutions with LlamaIndex\n",
    "\n",
    "# introduction\n",
    "- LlamaIndex is a framework for developing data-driven LLM applications, offering data ingestion, indexing, and querying tools.\n",
    "- RAG-based applications can be improved by focusing on building production-ready code with a focus on data considerations.\n",
    "- How embedding references and summaries in text chunks can significantly improve retrieval performance.\n",
    "- The capability of LLMs to infer metadata filters for structured retrieval.\n",
    "- Fine-tuning embedding representations in LLM applications to achieve optimal retrieval performance.\n",
    "\n",
    "# Challenges of RAG Systems\n",
    "\n",
    "- **Document Updates and Stored Vectors**: Ensure the up to date document and their vectors in db.\n",
    "- **Chunking and Data Distribution**: document chunk sizes matter for granular and redundant importance.\n",
    "- **Diverse Representations in Latent Space**: Representations for para of text, images and tables should be in different latent space.\n",
    "- **Compliance**: Regulations and private data handling should be kept with reliable and trustworthy deployment stratergy.\n",
    "\n",
    "# Optimization\n",
    "\n",
    "## Model Selection and Hybrid Retrieval\n",
    "- Selecting appropriate models for embedding and generations is critical.\n",
    "- minimize cost with cheap and efficient embeddings.\n",
    "- In retrieval system balancing latency with quality is essential.\n",
    "\n",
    "## CPU-Based Inference \n",
    "- Intel®'s advanced optimization technologies help with the efficient fine-tuning and inference of neural network models on CPUs. The 4th Gen Intel® Xeon® Scalable processors come with Intel® Advanced Matrix Extensions (Intel® AMX), an AI-enhanced acceleration feature. Each core of these processors includes integrated BF16 and INT8 accelerators.\n",
    "\n",
    "## Retrieval performance\n",
    "- Dividing docs into smaller independent chunks often leads to failure during document retrieval, as individual segments may lack the broader necessary context. Llama index proivdes advanced rag techniques such as Hierarchical and sentence window node parsers.\n",
    "- Advance data management tools can help organize, index and retrieve data more effectively.\n",
    "\n",
    "# The Role of the Retrieval Step\n",
    "\n",
    "The retriever role is mostly underestimated and its vital for RAG pipeline. Llama index provides a variety of retrival method. Below are some of the techniques:\n",
    "\n",
    "- Combining keyword + embedding search in a hybrid approach can enhance retrieval of specific queries. [link](https://docs.llamaindex.ai/en/stable/examples/query_engine/CustomRetrievers.html)\n",
    "- Metadata filtering can provide additional context and improve the performance of the RAG pipeline. [link](https://docs.llamaindex.ai/en/stable/examples/vector_stores/WeaviateIndexDemo.html#metadata-filtering)\n",
    "- Re-ranking orders the search results by considering the recency of data to the user’s input query. [link](https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/CohereRerank.html)\n",
    "- Indexing documents by summaries and retrieving relevant information within the document. [link](https://docs.llamaindex.ai/en/stable/examples/index_structs/doc_summary/DocSummary.html)\n",
    "\n",
    "Additionally, augmenting chunks with metadata will provide more context and enhance retrieval accuracy by defining node relationships between chunks for retrieval algorithms.\n",
    "\n",
    "# RAG Best Practices\n",
    "\n",
    "## Fine-tunning embedding model\n",
    "- Fine-tuning the embedding model involves several key steps (like the creation of the training set) to enhance the embedding performance.\n",
    "- Generate train set using an LLM which can produce batch of question and answers given a document.\n",
    "- It can yield 5-10% improvement.\n",
    "- Techniques like adjustment of embedding model, adaptor, routers to boost the overall efficiency of the pipeline. These techniques captures more impactful embedding representation, extracting deeper and more significant insights from the data.\n",
    "\n",
    "## Evaluation\n",
    "- Regularly monitoring the performance of your RAG pipeline is a recommended practice.\n",
    "- Response evaluation focuses on whether the response aligns with the retrieved context and the initial query and if it adheres to the reference answer or set guidelines.\n",
    "- A common method for assessing responses involves employing a proficient LLM, such as GPT-4.\n",
    "\n",
    "## Hybrid Search\n",
    "- Using a search with keyword lookup with additional context from embeddings can yield better results."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
